---

# Alpine linux kubernetes Longhorn storage
# configuring requirements:
# https://longhorn.io/docs/1.8.0/deploy/install/#installation-requirements

- name: Test connection with ping
  ansible.builtin.ping:

- name: Install open-iscsi,lsblk,findmnt,blkid,nfs-utils,jq,helm,py3-setuptools,py3-pip,wireguard-tools,py3-yaml,yq packages
  community.general.apk:
    name: open-iscsi,lsblk,findmnt,blkid,nfs-utils,jq,helm,py3-setuptools,py3-pip,wireguard-tools,py3-yaml,yq
    state: present

- name: Set local.d script to 'mount --make-rshared /'
  copy:
    dest: /etc/local.d/mount-make-rshared.start
    owner: root
    group: root
    mode: '0751'
    content: |
      #!/bin/bash
      /bin/mount --make-rshared /

- name: Execute /bin/mount --make-rshared /
  ansible.builtin.command: /bin/mount --make-rshared /

- name: Enable and start service iscsid
  ansible.builtin.service:
    name: iscsid
    enabled: yes
    state: started

- name: Run kubectl label nodes worker1 worker2 worker3 worker4 node.longhorn.io/create-default-disk=true
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml label nodes worker1 worker2 worker3 worker4 node.longhorn.io/create-default-disk=true
  register: kubectl_label_output
  when: inventory_hostname in groups.control1

- name: Print kubectl label nodes worker1 worker2 worker3 worker4 node.longhorn.io/create-default-disk=true output
  ansible.builtin.debug:
    var: kubectl_label_output.stdout_lines
  when: inventory_hostname in groups.control1

- name: Add the Longhorn Helm charts repository on control1
  kubernetes.core.helm_repository:
    name: longhorn
    repo_url: https://charts.longhorn.io
  when: inventory_hostname in groups.control1

- name: Copy files/longhorn_values.1.9.0.yaml to /home/p/longhorn_values.1.9.0.yaml
  copy:
    src:  files/longhorn_values.1.9.0.yaml
    dest: /home/p/longhorn_values.1.9.0.yaml
    owner: root
    group: root
    mode: '0640'
  when: inventory_hostname in groups.control1

# Longhorn v1.8.x does not support setting nodeSelector or tolerations for individual CSI sidecars
# (e.g., csi-attacher, csi-provisioner, csi-resizer and csi-snapshotter) via values.yaml.
# This is a known limitation documented in # GitHub issue #2664.
# (Setting this for longhornManager, longhornUI and longhornDriver works.)
# As a workaround, we use a Helm post renderer script to set CSI_NODE_SELECTOR env var for longhorn-driver-deployer.
# https://helm.sh/docs/topics/advanced/#post-rendering
- name: Copy files/longhorn-post-renderer.sh to /home/p/longhorn-post-renderer.sh
  copy:
    src:  files/longhorn-post-renderer.sh
    dest: /home/p/longhorn-post-renderer.sh
    owner: root
    group: root
    mode: '0740'
  when: inventory_hostname in groups.control1

- name: Deploy Longhorn Helm chart v1.9.0 with values loaded from longhorn_values.yaml
  kubernetes.core.helm:
    name: longhorn
    chart_ref: longhorn/longhorn
    chart_version: 1.9.0
    state: present
    create_namespace: true
    release_namespace: longhorn-system
    wait: true
    update_repo_cache: true
    values_files:
      - /home/p/longhorn_values.1.9.0.yaml
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    post_renderer: /home/p/longhorn-post-renderer.sh
  when: inventory_hostname in groups.control1

- name: Get kubectl get no
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get no -owide
  register: kubectl_getno_output
  when: inventory_hostname in groups.control1

- name: Print kubectl get no output
  ansible.builtin.debug:
    var: kubectl_getno_output.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Pause for 60 seconds
  ansible.builtin.pause:
    seconds: 60

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Ensure Longhorn Manager PDB
  ansible.builtin.shell: |
    /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml apply -f - <<EOF
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: longhorn-manager-pdb
      namespace: longhorn-system
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          app: longhorn-manager
    EOF
  register: ensure_longhorn_manager
  when: inventory_hostname in groups.control1

- name: Print Ensure Longhorn Manager PDB output
  ansible.builtin.debug:
    var: ensure_longhorn_manager.stdout_lines
  when: inventory_hostname in groups.control1

- name: Ensure Longhorn CSI Plugin PDB
  ansible.builtin.shell: |
    /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml apply -f - <<EOF
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: longhorn-csi-pdb
      namespace: longhorn-system
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          app: longhorn-csi-plugin
    EOF
  register: ensure_longhorn_csi_plugin
  when: inventory_hostname in groups.control1

- name: Print Ensure Longhorn CSI Plugin PDB output
  ansible.builtin.debug:
    var: ensure_longhorn_csi_plugin.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Pause for 60 seconds
  ansible.builtin.pause:
    seconds: 60

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get all -n longhorn-system
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get all -n longhorn-system -owide
  register: get_all
  when: inventory_hostname in groups.control1

- name: Print kubectl get all -n longhorn-system output
  ansible.builtin.debug:
    var: get_all.stdout_lines
  when: inventory_hostname in groups.control1

- name: Set local-path storageclass non-default
  ansible.builtin.command:
    cmd: |
      /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
  register: default_storageclass
  when: inventory_hostname in groups.control1

- name: Print kubectl get set local-path storageclass non-default output
  ansible.builtin.debug:
    var: default_storageclass.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get storageclass
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get storageclass
  register: get_storageclass
  when: inventory_hostname in groups.control1

- name: Print kubectl get storageclass output
  ansible.builtin.debug:
    var: get_storageclass.stdout_lines
  when: inventory_hostname in groups.control1
