---

# Alpine linux kubernetes Longhorn storage
# configuring requirements:
# https://longhorn.io/docs/1.8.0/deploy/install/#installation-requirements

- name: Test connection with ping
  ansible.builtin.ping:

- name: Install open-iscsi,lsblk,findmnt,blkid,nfs-utils,jq,helm,py3-setuptools,py3-pip,wireguard-tools,py3-yaml,yq packages
  community.general.apk:
    name: open-iscsi,lsblk,findmnt,blkid,nfs-utils,jq,helm,py3-setuptools,py3-pip,wireguard-tools,py3-yaml,yq
    state: present

# Ensure OpenRC 'local' service is enabled so local.d scripts run at boot
- name: Enable local service at default runlevel
  command: rc-update add local default
  args:
    creates: /etc/runlevels/default/local

# Provide a robust local.d script to enforce mount propagation each boot
- name: Install mount propagation script
  copy:
    dest: /etc/local.d/01-mount-rshared.start
    owner: root
    group: root
    mode: '0755'
    content: |
      #!/bin/sh
      # Ensure mount events from host propagate into containers
      /bin/mount --make-rshared /

      # Kubelet is its own mount on your nodes; flip it too
      if [ -d /var/lib/kubelet ]; then
        /bin/mount --make-rshared /var/lib/kubelet || true
      fi

      # Longhorn (optional but recommended)
      if [ -d /var/lib/longhorn ] || /bin/mkdir -p /var/lib/longhorn; then
        # Bind to itself so it becomes a mount point if it isn't already
        /bin/mountpoint -q /var/lib/longhorn || /bin/mount --bind /var/lib/longhorn /var/lib/longhorn || true
        /bin/mount --make-rshared /var/lib/longhorn || true
      fi

# 6) Apply immediately without reboot
- name: Run local scripts now
  command: rc-service local restart

- name: Enable and start service iscsid
  ansible.builtin.service:
    name: iscsid
    enabled: yes
    state: started

- name: Run kubectl label nodes worker1 worker2 worker3 worker4 worker5 node.longhorn.io/create-default-disk=true
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml label nodes worker1 worker2 worker3 worker4 worker5 node.longhorn.io/create-default-disk=true
  register: kubectl_label_output
  when: inventory_hostname in groups.control1

- name: Print kubectl label nodes worker1 worker2 worker3 worker4 worker5 node.longhorn.io/create-default-disk=true output
  ansible.builtin.debug:
    var: kubectl_label_output.stdout_lines
  when: inventory_hostname in groups.control1

- name: Pause for 10 seconds
  ansible.builtin.pause:
    seconds: 10

- name: Add the Longhorn Helm charts repository on control1
  kubernetes.core.helm_repository:
    name: longhorn
    repo_url: https://charts.longhorn.io
  when: inventory_hostname in groups.control1

- name: Copy files/longhorn_values.1.10.1.yaml to /home/p/longhorn_values.1.10.1.yaml
  copy:
    src:  files/longhorn_values.1.10.1.yaml
    dest: /home/p/longhorn_values.1.10.1.yaml
    owner: root
    group: root
    mode: '0640'
  when: inventory_hostname in groups.control1

# Longhorn v1.8.x does not support setting nodeSelector or tolerations for individual CSI sidecars
# (e.g., csi-attacher, csi-provisioner, csi-resizer and csi-snapshotter) via values.yaml.
# This is a known limitation documented in # GitHub issue #2664.
# (Setting this for longhornManager, longhornUI and longhornDriver works.)
# As a workaround, we use a Helm post renderer script to set CSI_NODE_SELECTOR env var for longhorn-driver-deployer.
# https://helm.sh/docs/topics/advanced/#post-rendering
# - name: Copy files/longhorn-post-renderer.sh to /home/p/longhorn-post-renderer.sh
  # copy:
    # src:  files/longhorn-post-renderer.sh
    # dest: /home/p/longhorn-post-renderer.sh
    # owner: root
    # group: root
    # mode: '0740'
  # when: inventory_hostname in groups.control1

- name: Deploy Longhorn Helm chart v1.10.1 with values loaded from longhorn_values.yaml
  kubernetes.core.helm:
    name: longhorn
    chart_ref: longhorn/longhorn
    chart_version: 1.10.1
    state: present
    create_namespace: true
    release_namespace: longhorn-system
    wait: true
    update_repo_cache: true
    values_files:
      - /home/p/longhorn_values.1.10.1.yaml
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  when: inventory_hostname in groups.control1
#    post_renderer: /home/p/longhorn-post-renderer.sh

- name: Get kubectl get no
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get no -owide
  register: kubectl_getno_output
  when: inventory_hostname in groups.control1

- name: Print kubectl get no output
  ansible.builtin.debug:
    var: kubectl_getno_output.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Pause for 60 seconds
  ansible.builtin.pause:
    seconds: 60

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Ensure Longhorn Manager PDB
  ansible.builtin.shell: |
    /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml apply -f - <<EOF
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: longhorn-manager-pdb
      namespace: longhorn-system
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          app: longhorn-manager
    EOF
  register: ensure_longhorn_manager
  when: inventory_hostname in groups.control1

- name: Print Ensure Longhorn Manager PDB output
  ansible.builtin.debug:
    var: ensure_longhorn_manager.stdout_lines
  when: inventory_hostname in groups.control1

- name: Ensure Longhorn CSI Plugin PDB
  ansible.builtin.shell: |
    /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml apply -f - <<EOF
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: longhorn-csi-pdb
      namespace: longhorn-system
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          app: longhorn-csi-plugin
    EOF
  register: ensure_longhorn_csi_plugin
  when: inventory_hostname in groups.control1

- name: Print Ensure Longhorn CSI Plugin PDB output
  ansible.builtin.debug:
    var: ensure_longhorn_csi_plugin.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Pause for 60 seconds
  ansible.builtin.pause:
    seconds: 60

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get all -n longhorn-system
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get all -n longhorn-system -owide
  register: get_all
  when: inventory_hostname in groups.control1

- name: Print kubectl get all -n longhorn-system output
  ansible.builtin.debug:
    var: get_all.stdout_lines
  when: inventory_hostname in groups.control1

- name: Set local-path storageclass non-default
  ansible.builtin.command:
    cmd: |
      /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
  register: default_storageclass
  when: inventory_hostname in groups.control1

- name: Print kubectl get set local-path storageclass non-default output
  ansible.builtin.debug:
    var: default_storageclass.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get storageclass
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get storageclass
  register: get_storageclass
  when: inventory_hostname in groups.control1

- name: Print kubectl get storageclass output
  ansible.builtin.debug:
    var: get_storageclass.stdout_lines
  when: inventory_hostname in groups.control1
