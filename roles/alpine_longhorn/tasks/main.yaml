---

# Alpine linux kubernetes Longhorn storage
# configuring requirements:
# https://longhorn.io/docs/1.8.0/deploy/install/#installation-requirements

- name: Test connection with ping
  ansible.builtin.ping:

- name: Install open-iscsi,lsblk,findmnt,blkid,nfs-utils,jq,helm,py3-setuptools,py3-pip,wireguard-tools,py3-yaml,yq packages
  community.general.apk:
    name: open-iscsi,lsblk,findmnt,blkid,nfs-utils,jq,helm,py3-setuptools,py3-pip,wireguard-tools,py3-yaml,yq
    state: present

- name: Set local.d script to 'mount --make-rshared /'
  copy:
    dest: /etc/local.d/mount-make-rshared.start
    owner: root
    group: root
    mode: '0751'
    content: |
      #!/bin/bash
      /bin/mount --make-rshared /

- name: Execute /bin/mount --make-rshared /
  ansible.builtin.command: /bin/mount --make-rshared /

- name: Enable and start service iscsid
  ansible.builtin.service:
    name: iscsid
    enabled: yes
    state: started

- name: Run kubectl label nodes worker1 worker2 worker3 worker4 node.longhorn.io/create-default-disk=true
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml label nodes worker1 worker2 worker3 worker4 node.longhorn.io/create-default-disk=true
  register: kubectl_label_output
  when: inventory_hostname in groups.control1

- name: Print kubectl label nodes worker1 worker2 worker3 worker4 node.longhorn.io/create-default-disk=true output
  ansible.builtin.debug:
    var: kubectl_label_output.stdout_lines
  when: inventory_hostname in groups.control1

- name: Add the Longhorn Helm charts repository on control1
  kubernetes.core.helm_repository:
    name: longhorn
    repo_url: https://charts.longhorn.io
  when: inventory_hostname in groups.control1

- name: Copy files/longhorn_values.yaml to /home/p/longhorn_values.yaml
  copy:
    src:  files/longhorn_values.yaml
    dest: /home/p/longhorn_values.yaml
    owner: root
    group: root
    mode: '0640'
  when: inventory_hostname in groups.control1

# Longhorn v1.8.x does not support setting nodeSelector or tolerations for individual CSI sidecars
# (e.g., csi-attacher, csi-provisioner, csi-resizer and csi-snapshotter) via values.yaml.
# This is a known limitation documented in # GitHub issue #2664.
# (Setting this for longhornManager, longhornUI and longhornDriver works.)
# As a workaround, we use a Helm post-renderer script too patch these.
# https://docs.ansible.com/ansible/latest/collections/kubernetes/core/helm_module.html#parameter-post_renderer
# - name: Copy Helm post-renderer script
  # copy:
    # src: files/patch-csi.sh
    # dest: /usr/local/bin/patch-csi.sh
    # mode: '0755'
  # when: inventory_hostname in groups.control1

- name: Deploy Longhorn Helm chart v1.8.1 with values loaded from template and CSI post-renderer
  kubernetes.core.helm:
    name: longhorn
    chart_ref: longhorn/longhorn
    chart_version: 1.8.1
    state: present
    create_namespace: true
    release_namespace: longhorn-system
    wait: true
    update_repo_cache: true
    values_files:
      - /home/p/longhorn_values.yaml
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  when: inventory_hostname in groups.control1
#    post_renderer: /usr/local/bin/patch-csi.sh

- name: Get kubectl get no
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get no -owide
  register: kubectl_getno_output
  when: inventory_hostname in groups.control1

- name: Print kubectl get no output
  ansible.builtin.debug:
    var: kubectl_getno_output.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Pause for 60 seconds
  ansible.builtin.pause:
    seconds: 60

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

# Longhorn v1.8.x does not support setting nodeSelector or tolerations for individual CSI sidecars
# (e.g., csi-attacher, csi-provisioner, csi-resizer and csi-snapshotter) via values.yaml.
# This is a known limitation documented in # GitHub issue #2664.
# (Setting this for longhornManager, longhornUI and longhornDriver works.)
# As a workaround, use kubectl strategic merge patches post-install to set these.

- name: Patch csi-attacher (2 replicas, control-plane nodes)
  ansible.builtin.command:
    cmd: >
      /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml
      -n longhorn-system patch deployment csi-attacher
      --type=strategic
      -p '{"spec":{"replicas":2,"template":{"spec":{"tolerations":[{"key":"node-role.kubernetes.io/control-plane","operator":"Exists","effect":"NoSchedule"},{"key":"node-role.kubernetes.io/master","operator":"Exists","effect":"NoSchedule"},{"key":"CriticalAddonsOnly","operator":"Exists","effect":"NoSchedule"}],"nodeSelector":{"node-role.kubernetes.io/control-plane":"true"}}}}}'
  register: patch_csi_attacher
  when: inventory_hostname in groups.control1

- name: Print patch csi-attacher output
  ansible.builtin.debug:
    var: patch_csi_attacher.stdout_lines
  when: inventory_hostname in groups.control1

- name: Patch csi-provisioner (2 replicas, control-plane nodes)
  ansible.builtin.command:
    cmd: >
      /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml
      -n longhorn-system patch deployment csi-provisioner
      --type=strategic
      -p '{"spec":{"replicas":2,"template":{"spec":{"tolerations":[{"key":"node-role.kubernetes.io/control-plane","operator":"Exists","effect":"NoSchedule"},{"key":"node-role.kubernetes.io/master","operator":"Exists","effect":"NoSchedule"},{"key":"CriticalAddonsOnly","operator":"Exists","effect":"NoSchedule"}],"nodeSelector":{"node-role.kubernetes.io/control-plane":"true"}}}}}'
  register: patch_csi_provisioner
  when: inventory_hostname in groups.control1

- name: Print patch csi-provisioner output
  ansible.builtin.debug:
    var: patch_csi_provisioner.stdout_lines
  when: inventory_hostname in groups.control1

- name: Patch csi-resizer (2 replicas, control-plane nodes)
  ansible.builtin.command:
    cmd: >
      /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml
      -n longhorn-system patch deployment csi-resizer
      --type=strategic
      -p '{"spec":{"replicas":2,"template":{"spec":{"tolerations":[{"key":"node-role.kubernetes.io/control-plane","operator":"Exists","effect":"NoSchedule"},{"key":"node-role.kubernetes.io/master","operator":"Exists","effect":"NoSchedule"},{"key":"CriticalAddonsOnly","operator":"Exists","effect":"NoSchedule"}],"nodeSelector":{"node-role.kubernetes.io/control-plane":"true"}}}}}'
  register: patch_csi_resizer
  when: inventory_hostname in groups.control1

- name: Print patch csi-resizer output
  ansible.builtin.debug:
    var: patch_csi_resizer.stdout_lines
  when: inventory_hostname in groups.control1

- name: Patch csi-snapshotter (2 replicas, control-plane nodes)
  ansible.builtin.command:
    cmd: >
      /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml
      -n longhorn-system patch deployment csi-snapshotter
      --type=strategic
      -p '{"spec":{"replicas":2,"template":{"spec":{"tolerations":[{"key":"node-role.kubernetes.io/control-plane","operator":"Exists","effect":"NoSchedule"},{"key":"node-role.kubernetes.io/master","operator":"Exists","effect":"NoSchedule"},{"key":"CriticalAddonsOnly","operator":"Exists","effect":"NoSchedule"}],"nodeSelector":{"node-role.kubernetes.io/control-plane":"true"}}}}}'
  register: patch_csi_snapshotter
  when: inventory_hostname in groups.control1

- name: Print patch csi-snapshotter output
  ansible.builtin.debug:
    var: patch_csi_snapshotter.stdout_lines
  when: inventory_hostname in groups.control1

- name: Ensure Longhorn Manager PDB
  ansible.builtin.shell: |
    /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml apply -f - <<EOF
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: longhorn-manager-pdb
      namespace: longhorn-system
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          app: longhorn-manager
    EOF
  register: ensure_longhorn_manager
  when: inventory_hostname in groups.control1

- name: Print Ensure Longhorn Manager PDB output
  ansible.builtin.debug:
    var: ensure_longhorn_manager.stdout_lines
  when: inventory_hostname in groups.control1

- name: Ensure Longhorn CSI Plugin PDB
  ansible.builtin.shell: |
    /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml apply -f - <<EOF
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: longhorn-csi-pdb
      namespace: longhorn-system
    spec:
      minAvailable: 1
      selector:
        matchLabels:
          app: longhorn-csi-plugin
    EOF
  register: ensure_longhorn_csi_plugin
  when: inventory_hostname in groups.control1

- name: Print Ensure Longhorn CSI Plugin PDB output
  ansible.builtin.debug:
    var: ensure_longhorn_csi_plugin.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Pause for 60 seconds
  ansible.builtin.pause:
    seconds: 60

- name: Get kubectl get pod --all-namespaces
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get pod --all-namespaces -owide
  register: get_pods
  when: inventory_hostname in groups.control1

- name: Print kubectl get po --all-namespaces output
  ansible.builtin.debug:
    var: get_pods.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get all -n longhorn-system
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get all -n longhorn-system -owide
  register: get_all
  when: inventory_hostname in groups.control1

- name: Print kubectl get all -n longhorn-system output
  ansible.builtin.debug:
    var: get_all.stdout_lines
  when: inventory_hostname in groups.control1

- name: Set local-path storageclass non-default
  ansible.builtin.command:
    cmd: |
      /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
  register: default_storageclass
  when: inventory_hostname in groups.control1

- name: Print kubectl get set local-path storageclass non-default output
  ansible.builtin.debug:
    var: default_storageclass.stdout_lines
  when: inventory_hostname in groups.control1

- name: Get kubectl get storageclass
  ansible.builtin.command:
    cmd: /usr/local/bin/kubectl --kubeconfig /etc/rancher/k3s/k3s.yaml get storageclass
  register: get_storageclass
  when: inventory_hostname in groups.control1

- name: Print kubectl get storageclass output
  ansible.builtin.debug:
    var: get_storageclass.stdout_lines
  when: inventory_hostname in groups.control1
